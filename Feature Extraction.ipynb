{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cf4e4ce",
   "metadata": {},
   "source": [
    "# FEATURES EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f417ae40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\ramiz\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ramiz\\anaconda3\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ramiz\\anaconda3\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\ramiz\\anaconda3\\lib\\site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\ramiz\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ramiz\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\ramiz\\anaconda3\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\ramiz\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ramiz\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f04e87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ecabcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the direction of a packet, either forward or backward\n",
    "def determine_direction(row):\n",
    "    if row['Rx Address'] in ['Random', 'Public']:\n",
    "        # the packet is received by the device, indicating backward direction\n",
    "        return 'backward'  \n",
    "    elif row['Tx Address'] in ['Random', 'Public']:\n",
    "        # the packet is transmitted by the device, indicating forward direction\n",
    "        return 'forward'\n",
    "    else:\n",
    "        return 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6ea01f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts features per advertising address and adds them as new column in a new CSV file\n",
    "def feature_advertising_address(input_file_path, output_file_path, N=5):\n",
    "    df = pd.read_csv(input_file_path, delimiter=';', error_bad_lines=False, encoding='ISO-8859-1')\n",
    "    # Remove packets where the \"label\" field is empty\n",
    "    df = df.dropna(subset=['label'])\n",
    "    \n",
    "    # Constants\n",
    "    grouped_adv = df.groupby('Advertising Address')\n",
    "    df_adv = df['Advertising Address']\n",
    "    total_length1 = grouped_adv['Length.1'].sum()\n",
    "    df['RSSI float'] = df['RSSI'].str.extract(r'(-?\\d+)').astype(float)\n",
    "\n",
    "    # Calculate duration for each advertising address group and add as new column\n",
    "    first_time_per_address = grouped_adv['Time'].min()\n",
    "    last_time_per_address = grouped_adv['Time'].max()\n",
    "    duration_per_address = last_time_per_address - first_time_per_address\n",
    "    df['Duration 1'] = df_adv.map(duration_per_address)\n",
    "    \n",
    "    # Number of Packets, Packets per Second and Time per Packet features\n",
    "    flow_size_per_address = grouped_adv.size()\n",
    "    df['Number of Packets 1'] = df_adv.map(flow_size_per_address)\n",
    "    df['Packets per Second 1'] = df['Number of Packets 1'] / df['Duration 1']\n",
    "    df['Time per Packet 1'] = df['Duration 1'] / df['Number of Packets 1']\n",
    "\n",
    "    # Bytes per Second feature\n",
    "    total_length_by_adv = df_adv.map(total_length1)\n",
    "    df['Bytes per Second 1'] = total_length_by_adv / df['Duration 1']\n",
    "\n",
    "    # Min, Max, Sum, Avg, Std, Var of RSSI\n",
    "    min_rssi = grouped_adv['RSSI float'].min()\n",
    "    max_rssi = grouped_adv['RSSI float'].max()\n",
    "    sum_rssi = grouped_adv['RSSI float'].sum()\n",
    "    avg_rssi = grouped_adv['RSSI float'].mean()\n",
    "    std_rssi = grouped_adv['RSSI float'].std()\n",
    "    var_rssi = grouped_adv['RSSI float'].var()\n",
    "    df['Min RSSI 1'] = df_adv.map(min_rssi)\n",
    "    df['Max RSSI 1'] = df_adv.map(max_rssi)\n",
    "    df['Sum RSSI 1'] = df_adv.map(sum_rssi)\n",
    "    df['Average RSSI 1'] = df_adv.map(avg_rssi)\n",
    "    df['Standard Deviation RSSI 1'] = df_adv.map(std_rssi)\n",
    "    df['Variance RSSI 1'] = df_adv.map(var_rssi)\n",
    "\n",
    "    # Min, Max, Sum, Avg, Std, Var of Packet Length\n",
    "    min_length1 = grouped_adv['Length.1'].min()\n",
    "    max_length1 = grouped_adv['Length.1'].max()\n",
    "    sum_length1 = grouped_adv['Length.1'].sum()\n",
    "    avg_length1 = grouped_adv['Length.1'].mean()\n",
    "    std_length1 = grouped_adv['Length.1'].std()\n",
    "    var_length1 = grouped_adv['Length.1'].var()\n",
    "    df['Min Packet Length 1'] = df_adv.map(min_length1)\n",
    "    df['Max Packet Length 1'] = df_adv.map(max_length1)\n",
    "    df['Sum Packet Length 1'] = df_adv.map(sum_length1)\n",
    "    df['Average Packet Length 1'] = df_adv.map(avg_length1)\n",
    "    df['Standard Deviation Packet Length 1'] = df_adv.map(std_length1)\n",
    "    df['Variance Packet Length 1'] = df_adv.map(var_length1)\n",
    "\n",
    "    # Min, Max, Sum, Avg, Std, Var of Length of payload\n",
    "    min_payload_volume = grouped_adv['Length of payload'].min()\n",
    "    max_payload_volume = grouped_adv['Length of payload'].max()\n",
    "    sum_payload_volume = grouped_adv['Length of payload'].sum()\n",
    "    avg_payload_volume = grouped_adv['Length of payload'].mean()\n",
    "    std_payload_volume = grouped_adv['Length of payload'].std()\n",
    "    var_payload_volume = grouped_adv['Length of payload'].var()\n",
    "    df['Min Payload Length 1'] = df_adv.map(min_payload_volume)\n",
    "    df['Max Payload Length 1'] = df_adv.map(max_payload_volume)\n",
    "    df['Sum Payload Length 1'] = df_adv.map(sum_payload_volume)\n",
    "    df['Average Payload Length 1'] = df_adv.map(avg_payload_volume)\n",
    "    df['Standard Deviation Payload Length 1'] = df_adv.map(std_payload_volume)\n",
    "    df['Variance Payload Length 1'] = df_adv.map(var_payload_volume)\n",
    "\n",
    "    # Min, Max, Sum, Avg, Std, Var of Delta time\n",
    "    min_delta_time = grouped_adv['Delta time ( end to start)'].min()\n",
    "    max_delta_time = grouped_adv['Delta time ( end to start)'].max()\n",
    "    sum_delta_time = grouped_adv['Delta time ( end to start)'].sum()\n",
    "    avg_delta_time = grouped_adv['Delta time ( end to start)'].mean()\n",
    "    std_delta_time = grouped_adv['Delta time ( end to start)'].std()\n",
    "    var_delta_time = grouped_adv['Delta time ( end to start)'].var()\n",
    "    df['Min Delta Time 1'] = df_adv.map(min_delta_time)\n",
    "    df['Max Delta Time 1'] = df_adv.map(max_delta_time)\n",
    "    df['Sum of Delta Time 1'] = df_adv.map(sum_delta_time)\n",
    "    df['Average Delta Time 1'] = df_adv.map(avg_delta_time)\n",
    "    df['Standard Deviation Delta Time 1'] = df_adv.map(std_delta_time)\n",
    "    df['Variance Delta Time 1'] = df_adv.map(var_delta_time)\n",
    "\n",
    "    # Packet Direction feature\n",
    "    df['Packet Direction'] = df.apply(determine_direction, axis=1)\n",
    "\n",
    "    # Total Number of Forward and Backward Packets features\n",
    "    forward_packet_count = grouped_adv.apply(lambda x: (x['Packet Direction'] == 'forward').sum())\n",
    "    backward_packet_count = grouped_adv.apply(lambda x: (x['Packet Direction'] == 'backward').sum())\n",
    "    df['Nr Forward Packets 1'] = df_adv.map(forward_packet_count)\n",
    "    df['Nr Backward Packets 1'] = df_adv.map(backward_packet_count)\n",
    "\n",
    "    # Average number of forward/backward packets\n",
    "    avg_forward_packet_count = grouped_adv.apply(lambda x: (x['Packet Direction'] == 'forward').mean())\n",
    "    avg_backward_packet_count = grouped_adv.apply(lambda x: (x['Packet Direction'] == 'backward').mean())\n",
    "    df['Average Nr Forward Packet 1'] = df_adv.map(avg_forward_packet_count)\n",
    "    df['Average Nr Backward Packet 1'] = df_adv.map(avg_backward_packet_count)\n",
    "\n",
    "    # Min, Max, Sum, Avg, Std, Var of delta time for forward/backward packets\n",
    "    min_forward_delta_time = grouped_adv.apply(lambda x: x[x['Packet Direction'] == 'forward']['Delta time ( end to start)'].min())\n",
    "    max_forward_delta_time = grouped_adv.apply(lambda x: x[x['Packet Direction'] == 'forward']['Delta time ( end to start)'].max())\n",
    "    sum_forward_delta_time = grouped_adv.apply(lambda x: x[x['Packet Direction'] == 'forward']['Delta time ( end to start)'].sum())\n",
    "    avg_forward_delta_time = grouped_adv.apply(lambda x: x[x['Packet Direction'] == 'forward']['Delta time ( end to start)'].mean())\n",
    "    std_forward_delta_time = grouped_adv.apply(lambda x: x[x['Packet Direction'] == 'forward']['Delta time ( end to start)'].std())\n",
    "    var_forward_delta_time = grouped_adv.apply(lambda x: x[x['Packet Direction'] == 'forward']['Delta time ( end to start)'].var())\n",
    "    min_backward_delta_time = grouped_adv.apply(lambda x: x[x['Packet Direction'] == 'backward']['Delta time ( end to start)'].min())\n",
    "    max_backward_delta_time = grouped_adv.apply(lambda x: x[x['Packet Direction'] == 'backward']['Delta time ( end to start)'].max())\n",
    "    sum_backward_delta_time = grouped_adv.apply(lambda x: x[x['Packet Direction'] == 'backward']['Delta time ( end to start)'].sum())\n",
    "    avg_backward_delta_time = grouped_adv.apply(lambda x: x[x['Packet Direction'] == 'backward']['Delta time ( end to start)'].mean())\n",
    "    std_backward_delta_time = grouped_adv.apply(lambda x: x[x['Packet Direction'] == 'backward']['Delta time ( end to start)'].std())\n",
    "    var_backward_delta_time = grouped_adv.apply(lambda x: x[x['Packet Direction'] == 'backward']['Delta time ( end to start)'].var())\n",
    "    df['Min Forward Delta Time 1'] = df_adv.map(min_forward_delta_time)\n",
    "    df['Max Forward Delta Time 1'] = df_adv.map(max_forward_delta_time)\n",
    "    df['Sum Forward Delta Time 1'] = df_adv.map(sum_forward_delta_time)\n",
    "    df['Avg Forward Delta Time 1'] = df_adv.map(avg_forward_delta_time)\n",
    "    df['Std Forward Delta Time 1'] = df_adv.map(std_forward_delta_time)\n",
    "    df['Var Forward Delta Time 1'] = df_adv.map(var_forward_delta_time)\n",
    "    df['Min Backward Delta Time 1'] = df_adv.map(min_backward_delta_time)\n",
    "    df['Max Backward Delta Time 1'] = df_adv.map(max_backward_delta_time)\n",
    "    df['Sum Backward Delta Time 1'] = df_adv.map(sum_backward_delta_time)\n",
    "    df['Avg Backward Delta Time 1'] = df_adv.map(avg_backward_delta_time)\n",
    "    df['Std Backward Delta Time 1'] = df_adv.map(std_backward_delta_time)\n",
    "    df['Var Backward Delta Time 1'] = df_adv.map(var_backward_delta_time)\n",
    "\n",
    "    # Save the new added features to a new CSV file\n",
    "    df.to_csv(output_file_path, index=False, sep=';', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7024470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv 20 minutes/labeled2\\2 hp, 1 apple laptop, bose & samsung headphones_labeled.csv\n",
      "csv 20 minutes/labeled2\\3 ipads_labeled.csv\n",
      "csv 20 minutes/labeled2\\4 airtags_labeled.csv\n",
      "csv 20 minutes/labeled2\\airfryer_labeled.csv\n",
      "csv 20 minutes/labeled2\\airpods, charge2 smartwatch, xiaomi smartphone_labeled.csv\n",
      "csv 20 minutes/labeled2\\apple smartwatch_labeled.csv\n",
      "csv 20 minutes/labeled2\\camera canon_labeled.csv\n",
      "csv 20 minutes/labeled2\\dell laptops_labeled.csv\n",
      "csv 20 minutes/labeled2\\google smartphone_labeled.csv\n",
      "csv 20 minutes/labeled2\\gopro corina_labeled.csv\n",
      "csv 20 minutes/labeled2\\gopro5.1_labeled.csv\n",
      "csv 20 minutes/labeled2\\gopro5_labeled.csv\n",
      "csv 20 minutes/labeled2\\gopro9_labeled.csv\n",
      "csv 20 minutes/labeled2\\LG tv_labeled.csv\n",
      "csv 20 minutes/labeled2\\lh wh 3 & 4 & linkbuds headphones_labeled.csv\n",
      "csv 20 minutes/labeled2\\mindi smartwatch_labeled.csv\n",
      "csv 20 minutes/labeled2\\mixer_labeled.csv\n",
      "csv 20 minutes/labeled2\\ramize mindi iphone, huawei smartphone, fitbit smartwatch_labeled.csv\n",
      "csv 20 minutes/labeled2\\samsung smartphone, oralB toothbrush, rollei, apple smartwatch_labeled.csv\n",
      "csv 20 minutes/labeled2\\smartwatch huawei_labeled.csv\n",
      "csv 20 minutes/labeled2\\wasserkocher ipad 20min_labeled.csv\n",
      "csv 20 minutes/labeled2\\ülkü_labeled.csv\n"
     ]
    }
   ],
   "source": [
    "directory_path = 'csv 20 minutes/labeled2/'\n",
    "output_directory = os.path.join(directory_path, 'features_final')\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "files = glob.glob(os.path.join(directory_path, '*_labeled.csv'))\n",
    "\n",
    "# For each file ending with \"_labeled\" do feature extraction and save as new CSV file with ending \"_features1\"\n",
    "for file in files:\n",
    "    file_name = os.path.basename(file)\n",
    "    output_file_path = os.path.join(output_directory, file_name.replace('.csv', '_features1.csv'))\n",
    "    feature_advertising_address(file, output_file_path)\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39972d44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2b5b7bb",
   "metadata": {},
   "source": [
    "# PUT ALL CSV TOGETHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d3460a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2 hp, 1 apple laptop, bose & samsung headphones_labeled_features1.csv', '3 ipads_labeled_features1.csv', '4 airtags_labeled_features1.csv', 'airfryer_labeled_features1.csv', 'airpods, charge2 smartwatch, xiaomi smartphone_labeled_features1.csv', 'apple smartwatch_labeled_features1.csv', 'camera canon_labeled_features1.csv', 'dell laptops_labeled_features1.csv', 'google smartphone_labeled_features1.csv', 'gopro corina_labeled_features1.csv', 'gopro5.1_labeled_features1.csv', 'gopro5_labeled_features1.csv', 'gopro9_labeled_features1.csv', 'LG tv_labeled_features1.csv', 'lh wh 3 & 4 & linkbuds headphones_labeled_features1.csv', 'mindi smartwatch_labeled_features1.csv', 'mixer_labeled_features1.csv', 'ramize mindi iphone, huawei smartphone, fitbit smartwatch_labeled_features1.csv', 'samsung smartphone, oralB toothbrush, rollei, apple smartwatch_labeled_features1.csv', 'smartwatch huawei_labeled_features1.csv', 'wasserkocher ipad 20min_labeled_features1.csv', 'ülkü_labeled_features1.csv']\n"
     ]
    }
   ],
   "source": [
    "directory = 'csv 20 minutes/labeled2/features_final'\n",
    "# Print all files ending with \"_features1\"\n",
    "csv_files = [file for file in os.listdir(directory) if file.endswith('_features1.csv')]\n",
    "print(csv_files)\n",
    "\n",
    "merged_df = pd.DataFrame()\n",
    "# iterate through each CSV file and put the\n",
    "    df = pd.read_csv(os.path.join(directory, csv_file), delimiter=';', error_bad_lines=False, encoding='ISO-8859-1')\n",
    "    df_with_labels = df[df['label'].notna()]\n",
    "    merged_df = pd.concat([merged_df, df_with_labels])\n",
    "\n",
    "# Save to a new CSV file\n",
    "merged_df.to_csv('csv 20 minutes/labeled2/features_final/together/all_data_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6021b6fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4494e8ed",
   "metadata": {},
   "source": [
    "Check if some columns are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6726873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if some added columns are the same\n",
    "def check_identical_columns(input_file):\n",
    "    df = pd.read_csv(input_file, delimiter=';', error_bad_lines=False, encoding='ISO-8859-1')\n",
    "    identical_columns = []\n",
    "    for i, col1 in enumerate(df.columns):\n",
    "        for j, col2 in enumerate(df.columns):\n",
    "            if i < j: \n",
    "                 # Check if columns have identical values\n",
    "                if df[col1].equals(df[col2]): \n",
    "                    identical_columns.append((col1, col2))\n",
    "\n",
    "    # Print the identical columns\n",
    "    if identical_columns:\n",
    "        print(\"Identical columns found:\")\n",
    "        for pair in identical_columns:\n",
    "            print(f\"{pair[0]} is identical to {pair[1]}\")\n",
    "    else:\n",
    "        print(\"No identical columns found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c687f8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identical columns found:\n",
      "Standard Deviation Packet Length 1 is identical to Standard Deviation Payload Length 1\n",
      "Variance Packet Length 1 is identical to Variance Payload Length 1\n"
     ]
    }
   ],
   "source": [
    "check_identical_columns(\"csv 20 minutes/labeled2/features_final/together/all_data_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39bd63c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
