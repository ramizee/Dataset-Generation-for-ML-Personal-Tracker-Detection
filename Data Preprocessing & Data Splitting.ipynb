{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7128c1c",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bdd250",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "216971b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, learning_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score, recall_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2f3261",
   "metadata": {},
   "source": [
    "The path to the file should be adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "551418c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv_file = 'csv 20 minutes/labeled2/features_final/together/all_data_final.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "22456ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows all numerical, categorial and boolean features\n",
    "def get_feature_types(df):\n",
    "    numerical_features = []\n",
    "    categorical_features = []\n",
    "    boolean_features = []\n",
    "    for column_name, column_type in df.dtypes.items():\n",
    "        if column_type in ['int64', 'float64']:\n",
    "            numerical_features.append(column_name)\n",
    "        elif column_type == 'object':\n",
    "            categorical_features.append(column_name)\n",
    "        elif column_type == 'bool':\n",
    "            boolean_features.append(column_name)\n",
    "\n",
    "    return numerical_features, categorical_features, boolean_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9de5be",
   "metadata": {},
   "source": [
    "Missing Values, Empty Ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7d32a35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists all columns which are more than 99% empty\n",
    "def list_empty_columns(csv_file):\n",
    "    df = pd.read_csv(csv_file, delimiter=';', encoding='ISO-8859-1')\n",
    "    missing_percentages = (df.isnull().sum() / len(df)) * 100\n",
    "    empty_columns = missing_percentages[missing_percentages > 99].index.tolist()\n",
    "    return empty_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "91695b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with more than 99% empty values:\n",
      "['Target Address', 'Address Type', 'Simultaneous LE and BR/EDR to Same Device Capable (Host).1', 'LE Supported By Host', 'OOB Data Present', 'Hash C', 'Custom UUID', 'BIG_Offset', 'BIG_Offset_Units', 'ISO_Interval', 'Num_BIS', 'NSE', 'BN', 'Sub_Interval', 'PTO', 'BIS_Spacing', 'IRC', 'Max_PDU', 'Reserved.1', 'Seed Access Address', 'SSP OOB Length', 'Malformed Packet']\n"
     ]
    }
   ],
   "source": [
    "empty_columns = list_empty_columns(input_csv_file)\n",
    "print(\"Columns with more than 99% empty values:\")\n",
    "print(empty_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6825ad",
   "metadata": {},
   "source": [
    "Columns with same values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9e1e62b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists all columns which have same values and are constants\n",
    "def list_same_value_columns(csv_file):\n",
    "    df = pd.read_csv(csv_file, delimiter=';', encoding='ISO-8859-1')\n",
    "    same_value_columns = []\n",
    "    for column in df.columns:\n",
    "        if df[column].nunique() == 1:\n",
    "            same_value_columns.append(column)\n",
    "    return same_value_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8fbb0bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with always the same values:\n",
      "['Length of packet', 'PHY', 'Access Address']\n"
     ]
    }
   ],
   "source": [
    "same_value_columns = list_same_value_columns(input_csv_file)\n",
    "print(\"Columns with always the same values:\")\n",
    "print(same_value_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4036d27e",
   "metadata": {},
   "source": [
    "Identical column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6646e0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists all columns which are identical to each other\n",
    "def check_identical_columns(input_file):\n",
    "    df = pd.read_csv(input_file, delimiter=';', error_bad_lines=False, encoding='ISO-8859-1')\n",
    "    columns_to_remove = set()\n",
    "    for i, col1 in enumerate(df.columns):\n",
    "        for j, col2 in enumerate(df.columns):\n",
    "            if i < j: \n",
    "                if df[col1].equals(df[col2]):\n",
    "                    columns_to_remove.add(col2)\n",
    "\n",
    "    columns_to_remove = list(columns_to_remove)\n",
    "\n",
    "    if columns_to_remove:\n",
    "        print(\"Columns to remove:\")\n",
    "        print(columns_to_remove)\n",
    "    else:\n",
    "        print(\"No identical columns found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "82291d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to remove:\n",
      "['Variance Payload Length 1', 'Standard Deviation Payload Length 1']\n"
     ]
    }
   ],
   "source": [
    "check_identical_columns(input_csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362f24be",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "53add6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(input_csv_file, delimiter=';', encoding='ISO-8859-1')\n",
    "\n",
    "# Drop columns with more than 99% empty values\n",
    "empty_columns = ['Target Address', 'Address Type', 'Simultaneous LE and BR/EDR to Same Device Capable (Host).1', \n",
    "                 'LE Supported By Host', 'OOB Data Present', 'Hash C', 'Custom UUID', 'BIG_Offset', 'BIG_Offset_Units', \n",
    "                 'ISO_Interval', 'Num_BIS', 'NSE', 'BN', 'Sub_Interval', 'PTO', 'BIS_Spacing', 'IRC', 'Max_PDU', \n",
    "                 'Reserved.1', 'Seed Access Address', 'SSP OOB Length', 'Malformed Packet']\n",
    "data = data.drop(empty_columns, axis=1)\n",
    "\n",
    "# Drop columns with constant values\n",
    "constant_columns = ['Length of packet', 'PHY', 'Access Address']\n",
    "data = data.drop(constant_columns, axis=1)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "unnecessary_columns = ['No.', 'Time', 'Protocol', 'RSSI', 'Packet Count']\n",
    "data = data.drop(unnecessary_columns, axis=1)\n",
    "\n",
    "# Drop unique identifier columns\n",
    "unique_columns = ['Advertising Address', 'Company ID', 'UUID 16', 'Device Name']\n",
    "data = data.drop(unique_columns, axis=1)\n",
    "\n",
    "# Drop identical columns\n",
    "identical_columns = ['Variance Payload Length 1']\n",
    "data = data.drop(identical_columns, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403d4df9",
   "metadata": {},
   "source": [
    "Print all categorial features which need to be transformer into numerical representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b5f84063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Packet Header', 'Type', 'Length', 'PDU Type', 'Info', 'Channel Selection Algorithm', 'Tx Address', 'Rx Address', 'Scanning Address', 'Simultaneous LE and BR/EDR to Same Device Capable (Host)', 'Simultaneous LE and BR/EDR to Same Device Capable (Controller)', 'BR/EDR Not Supported', 'LE General Discoverable Mode', 'LE Limited Discoverable Mode', 'Power Level (dBm)', 'Data', 'Custom UUID.1', 'Service Data', 'BD_ADDR', 'CRC', 'label', 'subcategory', 'Packet Direction']\n"
     ]
    }
   ],
   "source": [
    "numerical_features, categorical_features, boolean_features = get_feature_types(data)\n",
    "print(categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4065f76",
   "metadata": {},
   "source": [
    "Transform Packet Header column into Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ecbd349f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All values in 'packet header' column start with '0x': True\n",
      "All values in 'packet header' column have hexadecimal digits after '0x': True\n"
     ]
    }
   ],
   "source": [
    "# Packet Header column is categorial which starts with '0x' and by removing '0x' then it is numerical\n",
    "# So: check if all values in the Packet Header column start with '0x'\n",
    "all_start_with_0x = all(header.startswith('0x') for header in data['Packet Header'].unique())\n",
    "print(\"All values in Packet Header column start with '0x':\", all_start_with_0x)\n",
    "\n",
    "# Also: check if all values in Packet Header column have hexadecimal digits after '0x'\n",
    "hexidecimal_digit_pattern = re.compile(r'^0x[0-9a-fA-F]+$')\n",
    "all_match_pattern = all(hexidecimal_digit_pattern.match(header) for header in data['Packet Header'].unique())\n",
    "print(\"All values in Packet Header column have hexadecimal digits after '0x':\", all_match_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b359bb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of packet header column: int64\n"
     ]
    }
   ],
   "source": [
    "# Check if the transformed Packet Header column is in numerical representation\n",
    "data['Packet Header'] = data['Packet Header'].apply(lambda x: int(x[2:], 16))\n",
    "print(\"Data type of Packet Header column:\", data['Packet Header'].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389c632a",
   "metadata": {},
   "source": [
    "Transform Power Level column into Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "aa0cb421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Power Level column is categorial because of the ending 'dBm', so remove this ending to get the column in numerical representation\n",
    "def nonnumeric_power_level(data):\n",
    "    non_numeric_types = []\n",
    "    for index, row in data.iterrows():\n",
    "        power_level = row['Power Level (dBm)']\n",
    "        if isinstance(power_level, str):\n",
    "            try:\n",
    "                power_level = float(power_level)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        elif isinstance(power_level, int):\n",
    "            power_level = float(power_level)\n",
    "        if not isinstance(power_level, float):\n",
    "            non_numeric_type = type(power_level)\n",
    "            if non_numeric_type not in non_numeric_types:\n",
    "                non_numeric_types.append(non_numeric_type)\n",
    "            print(f\"Non-numeric value '{power_level}' with type '{non_numeric_type}' in row {index}, Power Level (dBm)\")\n",
    "            data.at[index, 'Power Level (dBm)'] = float(0)\n",
    "\n",
    "    print(\"\\nUnique non-numeric types in 'Power Level (dBm)' column:\")\n",
    "    print(non_numeric_types)\n",
    "    return data\n",
    "\n",
    "# some Power Level column values have not just one integer, so take the first integer if there are more than one integer\n",
    "def preprocess_power_level(data):\n",
    "    for index, row in data.iterrows():\n",
    "        power_level = row['Power Level (dBm)']\n",
    "        if isinstance(power_level, str):\n",
    "            parts = power_level.split(',')\n",
    "            try:\n",
    "                power_level = float(parts[0])\n",
    "            except ValueError:\n",
    "                pass\n",
    "            data.at[index, 'Power Level (dBm)'] = power_level\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5a6c6879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric value '26,-44' with type '<class 'str'>' in row 142702, Power Level (dBm)\n",
      "\n",
      "Unique non-numeric types in 'Power Level (dBm)' column:\n",
      "[<class 'str'>]\n"
     ]
    }
   ],
   "source": [
    "data = nonnumeric_power_level(data)\n",
    "data = preprocess_power_level(data)\n",
    "data['Power Level (dBm)'] = data['Power Level (dBm)'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861ece21",
   "metadata": {},
   "source": [
    "Handle Type and Length columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6bdfd741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of unique_types_set: 49\n",
      "['128-bit Service Class UUIDs', '128-bit Service Class UUIDs (incomplete)', '16-bit Service Class UUIDs', '16-bit Service Class UUIDs (incomplete)', '32-bit Service Class UUIDs', '32-bit Service Class UUIDs (incomplete)', '3D Information Data', 'Advertising Interval', 'Advertising Interval - long', 'Appearance', 'BD_ADDR', 'BIGInfo', 'Broadcast Code', 'Broadcast_Name', 'Channel Map Update Indication', 'Class Of Device', 'Device ID / Security Manager TK Value', 'Device Name', 'Device Name (shortened)', 'Flags', 'Indoor Positioning', 'LE Bluetooth Device Address', 'LE Role', 'LE Secure Connections Confirmation Value', 'LE Secure Connections Random Value', 'LE Supported Features', 'List of 128-bit Service Solicitation UUIDs', 'List of 16-bit Service Solicitation UUIDs', 'List of 32-bit Service Solicitation UUIDs', 'Manufacturer Specific', 'Mesh Beacon', 'Mesh Message', 'OOB Optional Data Length', 'PB-ADV', 'Peripheral Connection Interval Range', 'Public Target Address', 'Random Target Address', 'Resolvable Set Identifier', 'Security Manager Out of Band Flags', 'Service Data - 128 bit UUID', 'Service Data - 16 bit UUID', 'Service Data - 32 bit UUID', 'Simple Pairing Hash C', 'Simple Pairing Hash C-256', 'Simple Pairing Randomizer R', 'Simple Pairing Randomizer R-256', 'Transport Discovery Data', 'Tx Power Level', 'URI']\n"
     ]
    }
   ],
   "source": [
    "data['Type'].fillna('0x', inplace=True)\n",
    "unique_types_set = set()\n",
    "type_length_dict = {}\n",
    "# get unique values of all Type columns values\n",
    "for type_value in data['Type'].unique():\n",
    "    split_values = str(type_value).split(',')\n",
    "    for value in split_values:\n",
    "        value = value.strip()\n",
    "        if not value.startswith('0x'):\n",
    "            unique_types_set.add(value)\n",
    "\n",
    "print(\"Length of unique_types_set:\", len(unique_types_set))\n",
    "sorted_types = sorted(unique_types_set)\n",
    "print(sorted_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "45d2919a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new columns of Types and Lengths\n",
    "for column_name in sorted_types:\n",
    "    if column_name == 'BD_ADDR':\n",
    "        column_name = 'BD_ADDR2'\n",
    "    data[column_name] = 0\n",
    "    data[column_name + '_Length'] = 0\n",
    "    \n",
    "for index, row in data.iterrows():\n",
    "    types = row['Type'].split(',')\n",
    "    lengths = str(row['Length']).split(',')\n",
    "    \n",
    "    for t, l in zip(types, lengths):\n",
    "        # Update columns based on Types and lengths values\n",
    "        if t == 'BD_ADDR':\n",
    "            t = 'BD_ADDR2'\n",
    "        if t in sorted_types:\n",
    "            # Update the Type column with 1\n",
    "            data.at[index, t] = 1\n",
    "            # Update the length column with corresponding length value\n",
    "            data.at[index, t + '_Length'] = float(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f5e826d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove Type and Length columns\n",
    "handled_columns = ['Type', 'Length']\n",
    "data = data.drop(handled_columns, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b786db",
   "metadata": {},
   "source": [
    "Handle categorial columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a16fe1",
   "metadata": {},
   "source": [
    "Save subcategory for later data splitting !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4e0d3aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save subcategory for later data splitting !\n",
    "subcategory_index = data.columns.get_loc('subcategory')\n",
    "columns_to_convert = data.columns[subcategory_index + 1:]\n",
    "columns_to_convert = columns_to_convert[columns_to_convert != 'Packet Direction']\n",
    "for col in columns_to_convert:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "00b159c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PDU Type', 'Info', 'Channel Selection Algorithm', 'Tx Address', 'Rx Address', 'Scanning Address', 'Simultaneous LE and BR/EDR to Same Device Capable (Host)', 'Simultaneous LE and BR/EDR to Same Device Capable (Controller)', 'BR/EDR Not Supported', 'LE General Discoverable Mode', 'LE Limited Discoverable Mode', 'Data', 'Custom UUID.1', 'Service Data', 'BD_ADDR', 'CRC', 'label', 'subcategory', 'Packet Direction']\n"
     ]
    }
   ],
   "source": [
    "numerical_features, categorical_features, boolean_features = get_feature_types(data)\n",
    "print(categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cc1c5b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get for each categorial feature their unique value count\n",
    "def get_categorial_feature_types_count(df):\n",
    "    unique_counts = {}\n",
    "    for column_name, column_type in df.dtypes.items():\n",
    "        if column_type == 'object':\n",
    "            unique_counts[column_name] = df[column_name].nunique()\n",
    "\n",
    "    return unique_counts\n",
    "unique_counts = get_categorial_feature_types_count(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "16a1d551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 'PDU Type' has 7 unique values.\n",
      "Feature 'Info' has 24 unique values.\n",
      "Feature 'Channel Selection Algorithm' has 2 unique values.\n",
      "Feature 'Tx Address' has 2 unique values.\n",
      "Feature 'Rx Address' has 2 unique values.\n",
      "Feature 'Scanning Address' has 617 unique values.\n",
      "Feature 'Simultaneous LE and BR/EDR to Same Device Capable (Host)' has 10 unique values.\n",
      "Feature 'Simultaneous LE and BR/EDR to Same Device Capable (Controller)' has 10 unique values.\n",
      "Feature 'BR/EDR Not Supported' has 10 unique values.\n",
      "Feature 'LE General Discoverable Mode' has 9 unique values.\n",
      "Feature 'LE Limited Discoverable Mode' has 10 unique values.\n",
      "Feature 'Data' has 6819 unique values.\n",
      "Feature 'Custom UUID.1' has 47 unique values.\n",
      "Feature 'Service Data' has 1869 unique values.\n",
      "Feature 'BD_ADDR' has 382 unique values.\n",
      "Feature 'CRC' has 7487 unique values.\n",
      "Feature 'label' has 10 unique values.\n",
      "Feature 'subcategory' has 36 unique values.\n",
      "Feature 'Packet Direction' has 2 unique values.\n",
      "\n",
      "\n",
      "['Scanning Address', 'Data', 'Service Data', 'BD_ADDR', 'CRC']\n"
     ]
    }
   ],
   "source": [
    "# append features in the list which have more than 50 unique values\n",
    "too_much_categorial = []\n",
    "for feature, count in unique_counts.items():\n",
    "    print(f\"Feature '{feature}' has {count} unique values.\")\n",
    "    if count > 50:\n",
    "        too_much_categorial.append(feature)\n",
    "\n",
    "print('\\n')\n",
    "print(too_much_categorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "62763a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove categorial columns which have more than 50 unique values\n",
    "too_much_categorial_columns = ['Scanning Address', 'Data', 'Custom UUID.1', 'Service Data', 'BD_ADDR', 'CRC']\n",
    "data = data.drop(too_much_categorial_columns, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b489f274",
   "metadata": {},
   "source": [
    "This are the remaining categorial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "74645552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PDU Type', 'Info', 'Channel Selection Algorithm', 'Tx Address', 'Rx Address', 'Simultaneous LE and BR/EDR to Same Device Capable (Host)', 'Simultaneous LE and BR/EDR to Same Device Capable (Controller)', 'BR/EDR Not Supported', 'LE General Discoverable Mode', 'LE Limited Discoverable Mode', 'label', 'subcategory', 'Packet Direction']\n"
     ]
    }
   ],
   "source": [
    "numerical_features, categorical_features, boolean_features = get_feature_types(data)\n",
    "print(categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0319b8ae",
   "metadata": {},
   "source": [
    "SAVE FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "80faf3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv_file = 'csv 20 minutes/labeled2/features_final/together/all_data_final_before_one_hot_encoded.csv'\n",
    "data.to_csv(output_csv_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c4c114",
   "metadata": {},
   "source": [
    "LOAD FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc2a191",
   "metadata": {},
   "outputs": [],
   "source": [
    "before_one_hot_encoded_csv = 'csv 20 minutes/labeled2/features_final/together/all_data_final_before_one_hot_encoded.csv'\n",
    "data = pd.read_csv(before_one_hot_encoded_csv, delimiter=';', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302a213d",
   "metadata": {},
   "source": [
    "Data Transformation using One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ff97837e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_features(data, categorical_features):\n",
    "    # Remove 'label' and 'subcategory' columns from categorical features list\n",
    "    if 'label' in categorical_features:\n",
    "        categorical_features.remove('label')\n",
    "    if 'subcategory' in categorical_features:\n",
    "        categorical_features.remove('subcategory')\n",
    "    # Do one-hot encoding on the remaining categorical features\n",
    "    data = pd.get_dummies(data, columns=categorical_features, drop_first=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1cdcf05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = one_hot_encode_features(data, categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2c3cd0",
   "metadata": {},
   "source": [
    "Handle empty values: fill empty values with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1e271e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "31f0d62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 901623\n",
      "Rows with NaN values:\n"
     ]
    }
   ],
   "source": [
    "# Print total number of rows\n",
    "total_rows = data.shape[0]\n",
    "print(\"Total number of rows:\", total_rows)\n",
    "\n",
    "# Check if there are empty values\n",
    "empty_rows = data[data.isna().any(axis=1)]\n",
    "print(\"Rows with NaN values:\")\n",
    "for index, row in empty_rows.iterrows():\n",
    "    empty_columns = row[row.isna()].index.tolist()\n",
    "    print(f\"Row index {index}, empty columns: {empty_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1b0fe6",
   "metadata": {},
   "source": [
    "SAVE FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6d68e371",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv_file = 'csv 20 minutes/labeled2/features_final/together/all_data_final.csv'\n",
    "data.to_csv(output_csv_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4cd8de",
   "metadata": {},
   "source": [
    "LOAD FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1301b94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoded_csv = 'csv 20 minutes/labeled2/features_final/together/all_data_final.csv'\n",
    "data = pd.read_csv(one_hot_encoded_csv, delimiter=';', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852c0189",
   "metadata": {},
   "source": [
    "The remaining categorial columns are the label and subcategory columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ae5289e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['label', 'subcategory']\n"
     ]
    }
   ],
   "source": [
    "numerical_features, categorical_features, boolean_features = get_feature_types(data)\n",
    "print(categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a257e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c2709e8",
   "metadata": {},
   "source": [
    "# Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bceb7d",
   "metadata": {},
   "source": [
    "Split into X=features and y=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "328e1eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['label', 'subcategory'], axis=1)\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976be3b7",
   "metadata": {},
   "source": [
    "Save X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3c94958d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['csv 20 minutes/labeled2/features_final/together/y.pkl']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(X, 'csv 20 minutes/labeled2/features_final/together/X.pkl')\n",
    "joblib.dump(y, 'csv 20 minutes/labeled2/features_final/together/y.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2756a2a6",
   "metadata": {},
   "source": [
    "Load X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b68256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = joblib.load('csv 20 minutes/labeled2/features_final/together/X.pkl')\n",
    "y = joblib.load('csv 20 minutes/labeled2/features_final/together/y.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673fe564",
   "metadata": {},
   "source": [
    "## 80/20 Data Splitting Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a97af9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test datasets\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d4483d",
   "metadata": {},
   "source": [
    "SAVE TRAINING AND TEST DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8e041fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['csv 20 minutes/labeled2/features_final/together/y_test1.pkl']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(X_train1, 'csv 20 minutes/labeled2/features_final/together/X_train1.pkl')\n",
    "joblib.dump(X_test1, 'csv 20 minutes/labeled2/features_final/together/X_test1.pkl')\n",
    "joblib.dump(y_train1, 'csv 20 minutes/labeled2/features_final/together/y_train1.pkl')\n",
    "joblib.dump(y_test1, 'csv 20 minutes/labeled2/features_final/together/y_test1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6176596",
   "metadata": {},
   "source": [
    "LOAD TRAINING AND TEST DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2c9258",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = joblib.load('csv 20 minutes/labeled2/features_final/together/X_train1.pkl')\n",
    "X_test1 = joblib.load('csv 20 minutes/labeled2/features_final/together/X_test1.pkl')\n",
    "y_train1 = joblib.load('csv 20 minutes/labeled2/features_final/together/y_train1.pkl')\n",
    "y_test1 = joblib.load('csv 20 minutes/labeled2/features_final/together/y_test1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1944c0",
   "metadata": {},
   "source": [
    "## Alternative Data Splitting Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "890b27b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: Laptop\n",
      "Unique subcategories and their counts:\n",
      "  - Dell Laptop: 66187\n",
      "  - Hp Laptop: 54045\n",
      "  - Apple Laptop: 7561\n",
      "\n",
      "Label: Headphone\n",
      "Unique subcategories and their counts:\n",
      "  - Apple Headphone: 43456\n",
      "  - Sony Linkbuds Headphone: 43111\n",
      "  - Sony XM4 Headphone: 33335\n",
      "  - Sony XM3 Headphone: 15088\n",
      "  - Bose-2 Headphone: 4552\n",
      "  - Bose-1 Headphone: 3319\n",
      "  - Samsung Headphone: 1811\n",
      "  - Beats Headphone: 16\n",
      "\n",
      "Label: iPad\n",
      "Unique subcategories and their counts:\n",
      "  - Apple iPad: 74453\n",
      "\n",
      "Label: Airtag\n",
      "Unique subcategories and their counts:\n",
      "  - Apple Airtag: 16281\n",
      "\n",
      "Label: TV\n",
      "Unique subcategories and their counts:\n",
      "  - LG TV: 18657\n",
      "  - Samsung TV: 9467\n",
      "\n",
      "Label: Kitchen\n",
      "Unique subcategories and their counts:\n",
      "  - Kettle: 29756\n",
      "  - Airfryer: 4805\n",
      "  - Mixer: 3679\n",
      "\n",
      "Label: Smartwatch\n",
      "Unique subcategories and their counts:\n",
      "  - Apple Smartwatch: 82528\n",
      "  - Ericsson Smartwatch: 17371\n",
      "  - Huawei Smartwatch: 6567\n",
      "  - Fitbit Smartwatch: 2070\n",
      "  - Galaxy Smartwatch: 46\n",
      "\n",
      "Label: Camera\n",
      "Unique subcategories and their counts:\n",
      "  - Rollei Camera: 44376\n",
      "  - Canon: 41003\n",
      "  - GoPro9: 16162\n",
      "  - GoPro10: 14196\n",
      "  - GoPro5: 7774\n",
      "  - GoPro4: 7096\n",
      "\n",
      "Label: Smartphone\n",
      "Unique subcategories and their counts:\n",
      "  - Nokia Smartphone: 36086\n",
      "  - Huawei Smartphone: 19073\n",
      "  - Google Smartphone: 17235\n",
      "  - Samsung Smartphone: 16279\n",
      "  - Apple Smartphone: 13377\n",
      "\n",
      "Label: Bathroom\n",
      "Unique subcategories and their counts:\n",
      "  - Toothbrush: 129761\n",
      "  - AquaClean Shower Toilet: 1044\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get for each device type its unique subcategories and the number of packets of them\n",
    "def print_unique_subcategories_with_counts():\n",
    "    all_data_final = 'csv 20 minutes/labeled2/features_final/together/all_data_final.csv'\n",
    "    data2 = pd.read_csv(all_data_final, delimiter=';', encoding='ISO-8859-1', error_bad_lines=False)\n",
    "    \n",
    "    # Group by 'label' columns\n",
    "    labels = data2['label'].unique()\n",
    "    # For each label, get its unique subcategories and the number of packets of them\n",
    "    for label in labels:\n",
    "        subcategories = data2[data2['label'] == label]['subcategory'].value_counts()\n",
    "        print(f\"Label: {label}\")\n",
    "        print(\"Unique subcategories and their counts:\")\n",
    "        for subcategory, count in subcategories.items():\n",
    "            print(f\"  - {subcategory}: {count}\")\n",
    "        print()\n",
    "\n",
    "print_unique_subcategories_with_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4f1bffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test datasets\n",
    "def split_train_test_approach2(data):\n",
    "    train_data = pd.DataFrame(columns=data.columns)\n",
    "    test_data = pd.DataFrame(columns=data.columns)\n",
    "    \n",
    "    labels = data['label'].unique()\n",
    "    \n",
    "    for label in labels:\n",
    "        label_data = data[data['label'] == label]\n",
    "        subcategories = label_data['subcategory'].value_counts()\n",
    "        \n",
    "        # If there is only one subcategory of a device type then split randomly by 80/20 data splitting method\n",
    "        if len(subcategories) == 1:\n",
    "            train, test = train_test_split(label_data, test_size=0.2, random_state=42)\n",
    "        # Else: exclude the subcategory with the least number of packets from the training dataset\n",
    "        else:\n",
    "            subcategory_to_exclude = subcategories.idxmin()\n",
    "            remaining_data = label_data[label_data['subcategory'] != subcategory_to_exclude]\n",
    "            train, test_remain = train_test_split(remaining_data, test_size=0.2, random_state=42)\n",
    "            test_exclude = label_data[label_data['subcategory'] == subcategory_to_exclude]\n",
    "            test = pd.concat([test_remain, test_exclude])\n",
    "        \n",
    "        train_data = pd.concat([train_data, train])\n",
    "        test_data = pd.concat([test_data, test])\n",
    "    \n",
    "    total_count = len(data)\n",
    "    train_count = len(train_data)\n",
    "    test_count = len(test_data)\n",
    "    print(f\"Total data count: {total_count}\")\n",
    "    print(f\"Training data count: {train_count} ({(train_count / total_count) * 100:.2f}%)\")\n",
    "    print(f\"Testing data count: {test_count} ({(test_count / total_count) * 100:.2f}%)\")\n",
    "    \n",
    "    # For each label print their subcategories included in the training and test datasets\n",
    "    for label in labels:\n",
    "        train_subcategories = train_data[train_data['label'] == label]['subcategory'].unique()\n",
    "        test_subcategories = test_data[test_data['label'] == label]['subcategory'].unique()\n",
    "        print(f\"Label: {label}\")\n",
    "        print(f\"  Training subcategories: {', '.join(train_subcategories)}\")\n",
    "        print(f\"  Testing subcategories: {', '.join(test_subcategories)}\")\n",
    "        print()\n",
    "        \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e40b4909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data count: 901623\n",
      "Training data count: 687463 (76.25%)\n",
      "Testing data count: 214160 (23.75%)\n",
      "Label: Laptop\n",
      "  Training subcategories: Dell Laptop, Hp Laptop\n",
      "  Testing subcategories: Dell Laptop, Hp Laptop, Apple Laptop\n",
      "\n",
      "Label: Headphone\n",
      "  Training subcategories: Apple Headphone, Sony Linkbuds Headphone, Bose-1 Headphone, Sony XM4 Headphone, Sony XM3 Headphone, Samsung Headphone, Bose-2 Headphone\n",
      "  Testing subcategories: Apple Headphone, Sony XM3 Headphone, Sony XM4 Headphone, Bose-2 Headphone, Sony Linkbuds Headphone, Bose-1 Headphone, Samsung Headphone, Beats Headphone\n",
      "\n",
      "Label: iPad\n",
      "  Training subcategories: Apple iPad\n",
      "  Testing subcategories: Apple iPad\n",
      "\n",
      "Label: Airtag\n",
      "  Training subcategories: Apple Airtag\n",
      "  Testing subcategories: Apple Airtag\n",
      "\n",
      "Label: TV\n",
      "  Training subcategories: LG TV\n",
      "  Testing subcategories: LG TV, Samsung TV\n",
      "\n",
      "Label: Kitchen\n",
      "  Training subcategories: Kettle, Airfryer\n",
      "  Testing subcategories: Kettle, Airfryer, Mixer\n",
      "\n",
      "Label: Smartwatch\n",
      "  Training subcategories: Apple Smartwatch, Ericsson Smartwatch, Fitbit Smartwatch, Huawei Smartwatch\n",
      "  Testing subcategories: Apple Smartwatch, Ericsson Smartwatch, Fitbit Smartwatch, Huawei Smartwatch, Galaxy Smartwatch\n",
      "\n",
      "Label: Camera\n",
      "  Training subcategories: GoPro5, GoPro9, Rollei Camera, Canon, GoPro10\n",
      "  Testing subcategories: Rollei Camera, Canon, GoPro10, GoPro9, GoPro5, GoPro4\n",
      "\n",
      "Label: Smartphone\n",
      "  Training subcategories: Nokia Smartphone, Samsung Smartphone, Huawei Smartphone, Google Smartphone\n",
      "  Testing subcategories: Google Smartphone, Huawei Smartphone, Nokia Smartphone, Samsung Smartphone, Apple Smartphone\n",
      "\n",
      "Label: Bathroom\n",
      "  Training subcategories: Toothbrush\n",
      "  Testing subcategories: Toothbrush, AquaClean Shower Toilet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data_approach2, test_data_approach2 = split_train_test_approach2(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f4b052",
   "metadata": {},
   "source": [
    "SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ea9c6f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_approach2.to_csv('csv 20 minutes/labeled2/features_final/together/train_data_approach2.csv', index=False)\n",
    "test_data_approach2.to_csv('csv 20 minutes/labeled2/features_final/together/test_data_approach2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aaaccc",
   "metadata": {},
   "source": [
    "LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eabf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_approach2 = pd.read_csv('csv 20 minutes/labeled2/features_final/together/train_data_approach2.csv', delimiter=';', encoding='ISO-8859-1')\n",
    "test_data_approach2 = pd.read_csv('csv 20 minutes/labeled2/features_final/together/test_data_approach2.csv', delimiter=';', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4d59517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split data into training and test datasets\n",
    "X_train2 = train_data_approach2.drop(['label', 'subcategory'], axis=1)\n",
    "y_train2 = train_data_approach2['label']\n",
    "X_test2 = test_data_approach2.drop(['label', 'subcategory'], axis=1)\n",
    "y_test2 = test_data_approach2['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca0a4cc",
   "metadata": {},
   "source": [
    "SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "708f3b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['csv 20 minutes/labeled2/features_final/together/y_test2.pkl']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(X_train2, 'csv 20 minutes/labeled2/features_final/together/X_train2.pkl')\n",
    "joblib.dump(y_train2, 'csv 20 minutes/labeled2/features_final/together/y_train2.pkl')\n",
    "joblib.dump(X_test2, 'csv 20 minutes/labeled2/features_final/together/X_test2.pkl')\n",
    "joblib.dump(y_test2, 'csv 20 minutes/labeled2/features_final/together/y_test2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eb0f45",
   "metadata": {},
   "source": [
    "LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd3b494",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = joblib.load('csv 20 minutes/labeled2/features_final/together/X_train2.pkl')\n",
    "X_test2 = joblib.load('csv 20 minutes/labeled2/features_final/together/X_test2.pkl')\n",
    "y_train2 = joblib.load('csv 20 minutes/labeled2/features_final/together/y_train2.pkl')\n",
    "y_test2 = joblib.load('csv 20 minutes/labeled2/features_final/together/y_test2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a612e379",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
